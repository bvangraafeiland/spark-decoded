<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Spark Decoded</title>
    <link href="css/main.css" rel="stylesheet">
    <!--[if lt IE 9]>
    <script src="//oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
    <script src="//oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
</head>
<body>
<header class="jumbotron" role="banner">
    <div class="container text-center">
        <h1>Spark Decoded</h1>
        <p class="small"><em>Bastiaan van Graafeiland</em></p>
    </div>
</header>
<main>
    <section id="introduction">
        <h1>Introduction</h1>
        <p>Apache Spark is a cluster computing framework, designed for processing large amounts of data. It was created at the University of California, accompanied by <a href="http://www.eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-12.html">this paper</a>. Later, it was moved to the Apache Software Foundation.</p>
        <p>Due to its impressive performance, it rapidly gained an audience, and several organizations use Spark for (parts of) their applications, including Amazon, eBay and Yahoo! (<a href="https://cwiki.apache.org/confluence/display/SPARK/Powered+By+Spark">source</a>).</p>
        <p>In spite of all the attention Spark received, its inner workings are still hard to grasp. For starters, this is due to the paper not explaining it very thoroughly. Furthermore, although it is open sourced, the core functionality is hidden away rather deep down the rabbit hole, with high-level components such as schedulers and wrappers delegating calls to other classes. The documentation is focused on how to use it, rather than what happens under the hood. </p>
        <p>The goal of this project is to provide a simplified implementation of Spark's core that is easily understood, along with proper documentation that is considerably shorter than the paper. Unless indicated otherwise, code fragments in this article are taken from my own implementation.</p>
        <p>First, I will introduce some concepts that are part of Spark. Then in the next section, implementation details are explained.</p>
    </section>

    <section id="concepts">
        <h1>Concepts</h1>

        <h2>Resilient Distributed Datasets</h2>
        <p>Resilient Distributed Datasets, or <em>RDDs</em>, are the most important part of Spark. They are read-only wrappers around datasets that are created from Hadoop, text files, or any user-defined data source. By applying an operation to an RDD, a new RDD is formed rather than mutating the original one. Once all desired operations are applied, the final RDD can compute the result.</p>
        <p>RDDs are based on the MapReduce model, and build upon this model. As a result, any MapReduce operations can be ported to RDDs. It turns out that MapReduce by itself can already emulate any kind of distributed computation, so this extends to RDDs as well. The reason for this is that local computation can be done by the maps, while communication is achieved through the reduce steps. This way, each time step of a distributed computation can be taken on by MapReduce. From this, the generality of RDDs follows.</p>
        <p>A traditional issue is fault tolerance. This is generally solved by replicating data or logging updates, while operations allow for fine-grained operations on mutable data state. RDDs on the other hand only provide coarse grained transformations, where the same operation is applied to the entire dataset. This way, fault tolerance is easily achieved by keeping track of the operations performed on the data, rather than replicating the data itself. Another way to look at this is that the (much shorter) question is remembered, rather than the answer. On data loss (e.g. node failure), only the lost partitions need to be recalculated by other nodes.</p>
        <p>Of course, recomputing time consuming datasets after faults can be expensive. RDDs can therefore be persisted to internal memory or disk, so in case of a failure further down the line, the data doesn't need to be recomputed. This is also useful if one RDD is used as a parent for several other child RDDs.</p>
        <p>Formally, an RDD is defined by five properties:</p>
        <ul>
            <li>A list of partitions</li>
            <li>A function to compute a given partition</li>
            <li>A list of dependencies on other RDDs</li>
            <li>A partitioner, an object that assigns data to partitions</li>
            <li>A function that returns a list of preferred locations for a given partition, allowing faster access due to data locality</li>
        </ul>

        <h2>Partitions</h2>
        <p>A partition represents a subset of the data of an RDD, but generally has no reference to the data itself. It contains metadata that is needed for the computation of its data. When an RDD's data is computed, the partitions are distributed among the available nodes in the cluster, after which each node computes only their partitions. The amount of partitions in an RDD can be chosen by the user, but should usually at least be equal to the number of available nodes.</p>

        <h2>Partitioners</h2>
        <p>A partitioner assigns elements of an RDD consisting of key-value pairs to a partition by mapping a key value to a valid partition ID. Users can also implement their own custom partitioners.</p>

        <h2>Dependencies</h2>
        <p>Each RDD keeps track of the RDDs that it depends upon for computing its partitions. For "root" RDDs, constructed from an existing data source, this list is empty.</p>
        <p>In Spark, there are two different types of dependencies. First, there are <em>narrow</em> dependencies, where at most one partition of the child RDD depends upon a partition of the parent RDD. The other type is <em>wide</em> dependencies, where multiple partitions of the child may depend upon a single partition of the parent. An example of a narrow dependency is the <em>map</em> operation, which applies a provided function to each element of the dataset. Each partition of the parent RDD is mapped, resulting in the partitions of the child RDD. Therefore, to compute a given partition on the child RDD, only one partition of the parent RDD is required. An example of a wide dependency is the repartitioning of an RDD by a partitioner. After this, data from each partition in the parent RDD may be assigned to a single partition of the child RDD. Therefore, multiple partitions will be depending upon a single parent RDD partition.</p>
        <p>Following these definitions, narrow dependencies allow for more efficient computations, since multiple operations can be pipelined on a single partition, without having to compute the partition inbetween operations. Furthermore, recovery after a failure is easily achieved by computing the lost partitions in parallel on different nodes. Wide dependencies on the other hand, require all data from the parent to be computed to be shuffled across the child partitions. On node failure, the entire parent RDD needs to be recomputed, since all partitions contain data that is needed for the recalculation.</p>
    </section>

    <section id="implementation">
        <h1>Implementation</h1>

        <h2>RDDs</h2>
        <p>The base <code>RDD</code> class requires a context and its dependencies. Alternatively, another RDD can be passed and that RDD's context will be used, and set as the only dependency. An incrementing ID is obtained from the context and assigned to the RDD.</p>
        <pre><code class="scala">abstract class RDD[T](val context: SparkContext, deps: Seq[Dependency[_]])</code></pre>
        <p>The <code>partitions</code> and <code>dependencies</code> properties are computed only the first time they are needed, then saved internally, allowing for a time-consuming computation. Subclasses need to implement <code>getPartitions</code>. For dependencies this is optional, the value passed in the constructor is used by default.</p>
        <p>The <code>preferredLocations</code> method is left out in this implementation, because threads are used instead of true distribution. The partitioner is implemented as an <code>Option</code>, defaulting to <code>None</code> on the base class, because partitioners only make sense on key-value RDDs.</p>
        <p>Finally, the <code>compute</code> method is left for the subclass to implement, as this method is what separates different types of RDDs.</p>

        <h3>RDDs from existing data sources</h3>
        <p>To get an initial RDD, an existing data source can be used. This can be a file on the file system, or a <code>Seq</code> that has been computed already by some other means.</p>

        <h4>File system</h4>
        <p>Spark uses Hadoop's <code>InputFormat</code> class to read files from local storage or HDFS. This takes care of properly splitting the input file across partitions. For the sake of an easier explanation, I have simplified the functionality to text files only, without depending on Hadoop. This results in the following class signature:</p>
        <pre><code class="scala">class FileRDD(context: SparkContext, path: String, numPartitions: Int) extends RDD[String](context, Nil)</code></pre>
        <p>Note the dependencies are <code>Nil</code> since this is a root RDD. For dividing the file contents over the indicated number of partitions, the total number of lines is computed. To compute a partition, an iterator for the full text file is sliced, with boundaries depending on the partition's ID and the total number of lines divided by <code>numPartitions</code>.</p>

        <h4>Pre-computed sequence</h4>
        <p>A <code>ParallelCollectionRDD</code> is used for <code>Seq</code> input:</p>
        <pre><code class="scala">def parallelize[T](seq: Seq[T], numSlices: Int): RDD[T] = new ParallelCollectionRDD[T](this, seq, numSlices)</code></pre>
        <p>The dataset is then split into <code>numSlices</code> partitions of equal size. The partition class used, <code>ParallelCollectionPartition</code>, differs from the usual partition in that it contains a reference to the actual data, rather than just being a wrapper around the ID. From this, the following class signature follows:</p>
        <pre><code class="scala">class ParallelCollectionPartition[T](val rddId: Int, partitionId: Int, values: Seq[T]) extends Partition </code></pre>
        <p>Now, to compute a partition, all that is needed is to convert its <code>values</code> to an iterator.</p>

        <h3>Applying transformations</h3>
        
        <p>When a transformation is applied to an RDD, a new RDD will be returned rather than modifying the original one, respecting immutability. Nothing is computed yet, but the new RDD has the necessary information (e.g. function for a mapping) to compute its own data by transforming the data of its parent. This is accomplished by implementing the <code>getPartitions</code> and <code>compute</code> methods, and optionally <code>getDependencies</code> (by default, the dependencies are passed in the constructor). To illustrate, I will explain some transformations that are available.</p>

        <h4>Transformations available on all RDDs</h4>

        <h5>Union</h5>
        <pre><code class="scala">def union(other: RDD[T]): RDD[T] = new UnionRDD(context, Seq(this, other))</code></pre>
        <p>As you can see, this just creates a new <code>UnionRDD</code> instance and passes both RDDs to it. This class accepts a sequence of RDDs with the same type.</p>
        <p>Since its partitions are simply the union of the parents' partitions, the <code>getPartitions</code> method is implemented as such:</p>
        <pre><code class="scala">override def getPartitions: Array[Partition] = {
  val array = new Array[Partition](rdds.map(_.partitions.length).sum)
  var pos = 0
  for (rdd <- rdds; split <- rdd.partitions) {
    array(pos) = new UnionPartition(pos, rdd, split.index)
    pos += 1
  }
  array
}</code></pre>
        <p>Each parent partition is assigned to a <code>UnionPartition</code>. The parent partition and the parent itself are stored for use in the <code>compute</code> method:</p>
        <pre><code class="scala">override def compute(p: Partition): Iterator[T] = {
  val partition = p.asInstanceOf[UnionPartition[T]]
  partition.parent.iterator(partition.parentPartition)
}</code></pre>
        <p>Because a union does not change the data in any way, computing a partition is delegated to the corresponding parent.</p>

        <h5>Map and filter</h5>
        <pre><code class="scala">def map[U](f: T => U): RDD[U] = new MappedRDD[T,U](this, _.map(f))</code></pre>
        <pre><code class="scala">def filter(f: T => Boolean): RDD[T] = new MappedRDD[T,T](this, _.filter(f))</code></pre>
        <p>A <code>MappedRDD</code> requires a function that transforms an <code>Iterator</code> of type <code>T</code> to one of type <code>U</code>. The reason for this is to allow for a single type of RDD for both <code>map</code> and <code>filter</code>, among others. This function is obtained by passing the provided function to the iterator's <code>map</code> or <code>filter</code> method.</p>
        <p>A <code>MappedRDD</code> has the same partition as its parent. To compute a partition, it applies its function to the computed <code>Iterator</code> of its parent.</p>

        <h4>Transformations available on key-value RDDs</h4>
        <p>Some transformations are only available on RDDs that have key-value pairs. This is achieved by adding an implicit conversion to a <code>PairRDD</code> on the <code>RDD</code> object:</p>
        <pre><code class="scala">implicit def rddToPairRdd[K: ClassTag, V: ClassTag](rdd: RDD[(K,V)]): PairRDD[K,V] = new PairRDD(rdd)</code></pre>
        <p>The <code>PairRDD</code> class wraps around the RDD to provide the additional transformations and actions.</p>
        <p>An important part of key-value RDD functionality is the <code>Aggregator</code> class. It uses three functions:</p>
        <pre><code class="scala">class Aggregator[K,V,C] (createCombiner: V => C, mergeValue: (C,V) => C, mergeCombiners: (C,C) => C)</code></pre>
        <p>These functions are used in two methods. First, there is <code>combineValuesByKey</code>, which turns an iterator with <code>(K,V)</code> pairs into one with <code>(K,C)</code> pairs:</p>
        <pre><code class="scala">def combineValuesByKey(iter: Iterator[(K,V)]): Iterator[(K,C)] = {
  val combiners: mutable.HashMap[K,C] = new mutable.HashMap()
  iter.foreach(pair => {
    val key = pair._1
    val value = pair._2
    if (combiners contains key)
      combiners.update(key, mergeValue(combiners(key), value))
    else
      combiners.update(key, createCombiner(value))
  })
  combiners.toIterator
}</code></pre>
        <p>For each key, the first value of type <code>C</code> is obtained using the <code>createCombiner</code> function, then subsequent values for that key are merged into the result using the <code>mergeValue</code> function.</p>
        <p>The other aggregate method is <code>combineCombinersByKey</code>. It is similar to <code>combineValuesByKey</code>, but accepts an iterator that already contains <code>(K,C)</code> pairs, so <code>mergeCombiners</code> is used instead of <code>mergeValue</code>, and <code>createCombiner</code> is left out.</p>

        <h5>Combine by key</h5>
        <p>The <code>combineByKey</code> method is used as a base for several other methods of key-value RDDs. It is implemented as follows:</p>
        <pre><code class="scala">def combineByKey[C](createCombiner: V => C,
                    mergeValue: (C, V) => C,
                    mergeCombiners: (C, C) => C,
                    partitioner: Partitioner): RDD[(K,C)] = {
  val aggregator = new Aggregator[K, V, C](createCombiner, mergeValue, mergeCombiners)

  if (rdd.partitioner contains partitioner)
    rdd.mapPartitions(aggregator.combineValuesByKey, preservePartitioner = true)
  else
    new ShuffledRDD(rdd, partitioner, Some(aggregator))
}</code></pre>
        <p>The provided functions are wrapped in an <code>Aggregator</code>. If the RDD already contains a partitioner, the aggregator is used to map the combine function over each partition. Otherwise, a <code>ShuffledRDD</code> (explained in further detail in the Partitioners section) is used to repartition the RDD using the provided partitioner, resulting in a wide dependency.</p>
        <p>The result of the method is an RDD that is partitioned according to the provided partitioner, and values for each separate key have been combined into a single value.</p>
        
        <h5>Reduce by key</h5>
        <pre><code class="scala">def reduceByKey(f: (V,V) => V, numPartitions: Int): RDD[(K,V)] =
  combineByKey(identity, f, f, new HashPartitioner(numPartitions))</code></pre>
        <p>This method uses <code>combineByKey</code> to reduce the group of values for each key into a single value using the provided function <code>f</code>. Since the resulting RDD has the same type of values as this RDD, there is no need to transform a value into a combiner, so the identity function is used. A <code>HashPartitioner</code> (further explained in the Partitioners section) is used by default.</p>

        <h5>Aggregate by key</h5>
        <pre><code class="scala">def aggregateByKey[C](zeroValue: C, partitioner: Partitioner)(seqOp: (C, V) => C, comboOp: (C, C) => C): RDD[(K,C)] =
  combineByKey(v => seqOp(zeroValue, v), seqOp, comboOp, partitioner)</code></pre>    
        <p>This method is similar to <code>reduceByKey</code>, but it does not need a function to transform a value of type <code>V</code> into one of type <code>C</code>. Instead, it takes a "zero value" of type <code>C</code>, into which a given value can be merged using the provided merge function.</p>

        <h5>Group by key</h5>
        <pre><code class="scala">def groupByKey(numPartitions: Int): RDD[(K,Seq[V])] =
  combineByKey(v => Seq(v), (seq, v) => v +: seq, (seq1, seq2) => seq1 ++ seq2, new HashPartitioner(numPartitions))</code></pre>
        <p>Here, the values per key are combined into a sequence. Hence, in the resulting RDD, each key will appear only once. A single value is turned into a sequence with just one element, while a value can just be added to an existing sequence to merge it.</p>

        <h5>Partition by</h5>
        <pre><code class="scala">def partitionBy(partitioner: Partitioner): RDD[(K,V)] =
  if (rdd.partitioner contains partitioner)
    rdd
  else
    new ShuffledRDD[K,V,V](rdd, partitioner)</code></pre>
        <p>A <code>ShuffledRDD</code> is wrapped around the RDD in order to partition it using the provided partitioner. If the RDD was partitioned using the provided partitioner already, it can be returned as is.</p>

        <h3>Obtaining data with actions</h3>
        <!-- todo -->
        <p>Unlike transformations, an action on an RDD will actually cause it to compute its partitions. In order to do so, it will also cause its dependencies' partitions to be computed, starting a chain leading to the root RDD.</p>
        <p>As a base for all actions, <code>SparkContext</code> contains a method which computes (a subset of) an RDD's partitions, applies a given function to each resulting iterator, and calls another function on the result:</p>
        <pre><code class="scala">def runJob[T,U](rdd: RDD[T], func: Iterator[T] => U, partitions: Seq[Int], resultHandler: (Int, U) => Unit): Unit = {
  partitions.foreach(index => {
    val result = func(rdd.iterator(rdd.partitions(index)))
    resultHandler(index, result)
  })
}</code></pre>
        <p>In Spark, this method delegates calls to a scheduler, which will then balance the load over the nodes in the cluster. However, this logic is spread over many different classes and functions, to allow for proper delegation. For an easier explanation, I have implemented the logic in the <code>SparkContext</code> class directly.</p>
        <p>For convenience, a wrapper method is also provided which uses the above method to populate an array with the result of each iterator as its elements, using the <code>resultHandler</code> function:</p>
        <!-- todo -->
        <pre><code class="scala">def runJob[T,U](rdd: RDD[T], func: Iterator[T] => U): Array[U]</code></pre>
        <p>A number of actions use this method instead. Some available actions are:</p>

        <h4>Collect</h4>
        <p>Fetches all items in the dataset, and returns them as an array.</p>
        <h4>Count</h4>
        <p>Counts the number of elements in the dataset.</p>
        <h4>Reduce</h4>
        <p>Takes a function for reducing two elements to one, and returns a single element.</p>
        <h4>Take</h4>
        <p>Takes an integer <code>n</code>, and returns the first <code>n</code> results from the dataset.</p>

        <h2>Dependencies</h2>
        <!-- todo -->
        <h2>Partitioners</h2>
        <!-- todo -->

    </section>
</main>
<!-- Include all compiled plugins (below), or include individual files as needed -->
<script src="js/main.js"></script>
</body>
</html>