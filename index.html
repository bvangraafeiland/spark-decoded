<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <title>Spark Decoded</title>

    <!-- Bootstrap -->
    <link href="css/main.css" rel="stylesheet">

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
</head>
<body>
<header class="jumbotron">
    <div class="container">
        <h1>Spark Decoded</h1>
        <p class="small"><em>Bastiaan van Graafeiland</em></p>
    </div>
</header>
<main class="container">
    <h2>Introduction</h2>
    <p>Apache Spark is a cluster computing framework, designed for processing large amounts of data. It was created at the University of California, accompanied by <a href="http://www.eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-12.html">this paper</a>. Later, it was moved to the Apache Software Foundation. </p>
    <p>Due to its impressive performance, it rapidly gained an audience, and several organizations use Spark for (parts of) their applications, including Amazon, eBay and Yahoo! (<a href="https://cwiki.apache.org/confluence/display/SPARK/Powered+By+Spark">source</a>).</p>
    <p>In spite of all the attention Spark received, its inner workings are rather hard to grasp. For starters, this is due to the paper not explaining it very thoroughly. Furthermore, although it is open sourced, the core functionality is hidden away rather deep down the rabbit hole, in high-level components such as schedulers and wrapper classes. The documentation is focused on how to use it, rather than what happens under the hood. </p>
    <p>The goal of this project is to provide a simplified implementation of Spark's core that is easily understood, along with proper documentation that is considerably shorter than the paper. Unless indicated otherwise, code fragments in this article are taken from my own implementation.</p>
    <p>First, I will introduce the different kinds of components used in Spark. Then, in the next section, the interaction between the components will be explained.</p>

    <h2>Concepts</h2>
    <p>In this section, I will introduce the core concepts of Spark.</p>

    <h3>Resilient Distributed Datasets</h3>
    <p>Resilient Distributed Datasets, or RDDs, are the most important part of Spark. They are read-only wrappers around datasets that are created from Hadoop, text files, or any user-defined data source. By applying an operation to an RDD, a new RDD is formed rather than mutating the original one.</p>
    <p>RDDs are based on the MapReduce model, and build upon this model. As a result, any MapReduce operations can be ported to RDDs. It turns out that MapReduce by itself can already emulate any kind of distributed computation, so this extends to RDDs as well. The reason for this is that local computation can be done by the maps, while communication is achieved through the reduce steps. This way, each time step of a distributed computation can be taken on by MapReduce. From this, the generality of RDDs follows.</p>
    <p>A traditional issue is fault tolerance. This is generally solved by replicating data or logging updates, while operations allow for fine-grained operations on mutable data state. RDDs on the other hand only provide coarse grained transformations, where the same operation is applied to the entire dataset. This way, fault tolerance is easily achieved by logging only the operations performed on the data. On data loss, only the lost partitions need to be recalculated. When a node fails, another node can take over its partitions.</p>
    <p>Formally, an RDD is defined by five properties:</p>
    <ul>
        <li>A list of partitions</li>
        <li>A function to compute a given partition</li>
        <li>A list of dependencies on other RDDs</li>
        <li>A partitioner, an object that assigns data to partitions</li>
        <li>A function that returns a list of preferred locations for a given partition, allowing faster access due to data locality</li>
    </ul>

    <h3>Dependencies</h3>
    <p>Each RDD stores a list of parent RDDs that it depends upon for computing its partitions. This list can also be empty, for example in RDDs constructed from an in-memory array.</p>

    <h3>Partitions</h3>
    <p>A partition represents a subset of the data of an RDD. When running computations, the partitions are distributed among the available nodes in the cluster, after which each node computes only their partitions.</p>
    <p>Internally, a partition only needs to contain an ID, and any other information required for computing the partition's data, so it is just a pointer.</p>

    <h3>Partitioners</h3>
    <p>A partitioner assigns elements of an RDD consisting of key-value pairs to a partition by mapping a key value to a valid partition ID.</p>
    <p>A partition can be computed by calling the compute method on an RDD, resulting in an Iterator containing the requested data.</p>

    <h3>Transformations</h3>
    <p>When a transformation is applied to an RDD, a new RDD will be returned rather than modifying the original one, allowing for immutability. Upon applying the transformation, nothing is yet computed.</p>

    <h3>Actions</h3>
    <p>Unlike transformations, an action on an RDD will actually compute (a subset of) its partitions, by applying the stored transformations to the raw data of the root RDD. I will discuss several actions here.</p>
    <p><strong>Collect</strong> fetches all items in the dataset, and returns them as an array.</p>
    <p><strong>Count</strong> counts the number of elements in the dataset.</p>
    <p><strong>Reduce</strong> takes a function for reducing two elements to one, and returns a single element.</p>
    <p><strong>Take</strong> takes an integer <code>n</code>, and returns the first <code>n</code> results from the dataset.</p>
    <p>Now, for all of these, a number of the RDD's partitions will have to be computed. Therefore, it would be desirable to have a function that is general enough to provide implementations for all actions. Such a function could have a signature like this:</p>
    <pre><code class="scala">def runJob[T,U: ClassTag](rdd: RDD[T], func: Iterator[T] => U): Array[U]</code></pre>
    <p>What it should do is apply the given function <code>func</code> to each computed partition of the given RDD to obtain a result of type <code>U</code>, then combine these results into an array.</p>

</main>
<!-- Include all compiled plugins (below), or include individual files as needed -->
<script src="js/bundle.js"></script>
</body>
</html>