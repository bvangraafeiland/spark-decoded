<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <title>Spark Decoded</title>

    <!-- Bootstrap -->
    <link href="css/main.css" rel="stylesheet">

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
    <script src="//oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
    <script src="//oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
</head>
<body>
<header class="jumbotron" role="banner">
    <div class="container text-center">
        <h1>Spark Decoded</h1>
        <p class="small"><em>Bastiaan van Graafeiland</em></p>
    </div>
</header>
<main>
    <section id="introduction">
        <h1>Introduction</h1>
        <p>Apache Spark is a cluster computing framework, designed for processing large amounts of data. It was created at the University of California, accompanied by <a href="http://www.eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-12.html">this paper</a>. Later, it was moved to the Apache Software Foundation.</p>
        <p>Due to its impressive performance, it rapidly gained an audience, and several organizations use Spark for (parts of) their applications, including Amazon, eBay and Yahoo! (<a href="https://cwiki.apache.org/confluence/display/SPARK/Powered+By+Spark">source</a>).</p>
        <p>In spite of all the attention Spark received, its inner workings are still hard to grasp. For starters, this is due to the paper not explaining it very thoroughly. Furthermore, although it is open sourced, the core functionality is hidden away rather deep down the rabbit hole, in high-level components such as schedulers and wrapper classes. The documentation is focused on how to use it, rather than what happens under the hood. </p>
        <p>The goal of this project is to provide a simplified implementation of Spark's core that is easily understood, along with proper documentation that is considerably shorter than the paper. Unless indicated otherwise, code fragments in this article are taken from my own implementation.</p>
        <p>First, I will introduce some concepts that are part of Spark. Then in the next section, implementation details are explained.</p>
    </section>

    <section id="concepts">
        <h1>Concepts</h1>

        <h2>Resilient Distributed Datasets</h2>
        <p>Resilient Distributed Datasets, or RDDs, are the most important part of Spark. They are read-only wrappers around datasets that are created from Hadoop, text files, or any user-defined data source. By applying an operation to an RDD, a new RDD is formed rather than mutating the original one.</p>
        <p>RDDs are based on the MapReduce model, and build upon this model. As a result, any MapReduce operations can be ported to RDDs. It turns out that MapReduce by itself can already emulate any kind of distributed computation, so this extends to RDDs as well. The reason for this is that local computation can be done by the maps, while communication is achieved through the reduce steps. This way, each time step of a distributed computation can be taken on by MapReduce. From this, the generality of RDDs follows.</p>
        <p>A traditional issue is fault tolerance. This is generally solved by replicating data or logging updates, while operations allow for fine-grained operations on mutable data state. RDDs on the other hand only provide coarse grained transformations, where the same operation is applied to the entire dataset. This way, fault tolerance is easily achieved by logging only the operations performed on the data. On data loss, only the lost partitions need to be recalculated. When a node fails, another node can take over its partitions.</p>
        <p>Of course, recomputing time consuming datasets after faults can be expensive. RDDs can therefore be persisted to internal memory or disk, so in case of a failure further down the line, the data doesn't need to be recomputed. This is also useful if one RDD is used as a parent for several other child RDDs.</p>
        <p>Formally, an RDD is defined by five properties:</p>
        <ul>
            <li>A list of partitions</li>
            <li>A function to compute a given partition</li>
            <li>A list of dependencies on other RDDs</li>
            <li>A partitioner, an object that assigns data to partitions</li>
            <li>A function that returns a list of preferred locations for a given partition, allowing faster access due to data locality</li>
        </ul>

        <h2>Partitions</h2>
        <p>A partition represents a subset of the data of an RDD. When an RDD's data is computed, the partitions are distributed among the available nodes in the cluster, after which each node computes only their partitions.</p>
        <p>Internally, a partition only needs to contain an ID, so at its base, it is just a wrapper. Specific partition classes may also contain additional data that is needed for computation. A partition can be computed by calling the compute method on an RDD and passing the partition, resulting in an Iterator containing the requested data.</p>

        <h2>Dependencies</h2>
        <p>Each RDD stores a list of parent RDDs that it depends upon for computing its partitions. For "root" RDDs, constructed from an existing data source, this list is empty.</p>

        <h2>Partitioners</h2>
        <p>A partitioner assigns elements of an RDD consisting of key-value pairs to a partition by mapping a key value to a valid partition ID. Users can also implement their own custom partitioners.</p>

    </section>

    <section id="implementation">
        <h1>Implementation</h1>

        <h2>RDDs</h2>
        <p>The base <code>RDD</code> class requires a context and its dependencies. Alternatively, another RDD can be passed and that RDD's context will be used, and set as the only dependency. An incrementing ID is obtained from the context and assigned to the RDD.</p>
        <pre><code class="scala">abstract class RDD[T](val context: SparkContext, deps: Seq[Dependency[_]])</code></pre>
        <p>The <code>partitions</code> and <code>dependencies</code> properties are computed only the first time they are needed, then saved internally, allowing for a time-consuming computation. Subclasses need to implement <code>getPartitions</code>. For dependencies this is optional, the value passed in the constructor is used by default.</p>
        <p>The <code>preferredLocations</code> method is left out in this implementation, because threads are used instead of true distribution. The partitioner is implemented as an <code>Option</code>, defaulting to <code>None</code> on the base class, because partitioners only make sense on key-value RDDs.</p>
        <p>Finally, the <code>compute</code> method is left for the subclass to implement, as this method is what separates different types of RDDs.</p>

        <h3>RDDs from existing data sources</h3>
        <p>To get an initial RDD, an existing data source can be used. This can be a file on the file system, or a <code>Seq</code> that has been computed already by some other means.</p>

        <h4>File system</h4>
        <p>Spark uses Hadoop's <code>InputFormat</code> class to read files from local storage or HDFS. This takes care of properly splitting the input file across partitions. For the sake of an easier explanation, I have simplified the functionality to text files only, without depending on Hadoop. This results in the following class signature:</p>
        <pre><code class="scala">class FileRDD(context: SparkContext, path: String, numPartitions: Int) extends RDD[String](context, Nil)</code></pre>
        <p>Note the dependencies are <code>Nil</code> since this is a root RDD. For dividing the file contents over the indicated number of partitions, the total number of lines is computed. To compute a partition, an iterator for the full text file is sliced, with boundaries depending on the partition's ID and the total number of lines divided by <code>numPartitions</code>.</p>

        <h4>Computed sequence</h4>
        <p>A <code>ParallelCollectionRDD</code> is used for <code>Seq</code> input:</p>
        <pre><code class="scala">def parallelize[T](seq: Seq[T], numSlices: Int): RDD[T] = new ParallelCollectionRDD[T](this, seq, numSlices)</code></pre>
        <p>The dataset is then split into <code>numSlices</code> partitions of equal size. The partition class used, <code>ParallelCollectionPartition</code>, differs from the usual partition in that it contains a reference to the actual data, rather than just being a wrapper around the ID. From this, the following class signature follows:</p>
        <pre><code class="scala">class ParallelCollectionPartition[T](val rddId: Int, partitionId: Int, values: Seq[T]) extends Partition </code></pre>
        <p>Now, to compute a partition, all that is needed is to convert its <code>values</code> to an iterator.</p>

        <h3>Transformations</h3>
        <p>When a transformation is applied to an RDD, a new RDD will be returned rather than modifying the original one, allowing for immutability. Upon applying the transformation, nothing is yet computed. To illustrate, I will explain some transformations that are available.
        <h4>Union</h4>
        <pre><code class="scala">def union(other: RDD[T]): RDD[T] = new UnionRDD(context, Seq(this, other))</code></pre>
        <p>As you can see, this just creates a new <code>UnionRDD</code> instance and passes both RDDs to it.</p>
        <h4>Map</h4>
        <pre><code class="scala">def map[U](f: T => U): RDD[U] = new MappedRDD[T,U](this, iter => iter.map(f))</code></pre>
        <h4>Filter</h4>
        <pre><code class="scala">def filter(f: T => Boolean): RDD[T] = new MappedRDD[T,T](this, iter => iter.filter(f))</code></pre>
        <p>Some transformations are only available on RDDs that have key-value pairs. This is achieved by adding an implicit conversion to a <code>PairRDD</code> on the <code>RDD</code> object:</p>
        <pre><code class="scala">implicit def rddToPairRdd[K: ClassTag, V: ClassTag](rdd: RDD[(K,V)]): PairRDD[K,V] = new PairRDD(rdd)</code></pre>
        <h4>Combine by key (reduce, group)</h4>
        <h4>Partition by</h4>

        <h3>Actions</h3>
        <p>Unlike transformations, an action on an RDD will actually cause it to compute its partitions. In order to do so, it will also cause its dependencies' partitions to be computed, starting a chain leading to the root RDD.</p>
        <p>As a base for all actions, <code>SparkContext</code> contains a method which computes (a subset of) an RDD's partitions, applies a given function to each resulting iterator, and calls another function on the result:</p>
        <pre><code class="scala">def runJob[T,U](rdd: RDD[T], func: Iterator[T] => U, partitions: Seq[Int], resultHandler: (Int, U) => Unit): Unit = {
  partitions.foreach(index => {
    val result = func(rdd.compute(rdd.partitions(index)))

    resultHandler(index, result)
  })
}</code></pre>
        <p>In Spark, this method delegates calls to a scheduler, which will then balance the load over the nodes in the cluster. However, this logic is spread over many different classes and functions, to allow for proper delegation. For an easier explanation, I have implemented the logic in the <code>SparkContext</code> class directly.</p>
        <p>For convenience, a wrapper method is also provided which uses the above method to return an array with the result of each iterator as its elements:</p>
        <pre><code class="scala">def runJob[T,U](rdd: RDD[T], func: Iterator[T] => U): Array[U]</code></pre>
        <p>A number of actions use this method instead. Some available actions are:</p>

        <h4>Collect</h4>
        <p>Fetches all items in the dataset, and returns them as an array.</p>
        <h4>Count</h4>
        <p>Counts the number of elements in the dataset.</p>
        <h4>Reduce</h4>
        <p>Takes a function for reducing two elements to one, and returns a single element.</p>
        <h4>Take</h4>
        <p>Takes an integer <code>n</code>, and returns the first <code>n</code> results from the dataset.</p>

        <h2>Dependencies</h2>

        <h2>Partitioners</h2>

    </section>
</main>
<!-- Include all compiled plugins (below), or include individual files as needed -->
<script src="js/main.js"></script>
</body>
</html>